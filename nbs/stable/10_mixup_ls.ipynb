{
  "cells" : [
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "%install-location $cwd\/swift-install\n",
        "%install '.package(path: \"$cwd\/FastaiNotebook_09_optimizer\")' FastaiNotebook_09_optimizer"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "\/\/ export\n",
        "import Path\n",
        "import TensorFlow"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "import FastaiNotebook_09_optimizer"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "%include \"EnableIPythonDisplay.swift\"\n",
        "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
      ],
      "outputs" : [
        
      ]
    },
    {
      "metadata" : {
        
      },
      "cell_type" : "markdown",
      "source" : [
        "## Load data"
      ]
    },
    {
      "metadata" : {
        
      },
      "source" : [
        "\/\/TODO: switch to imagenette when possible to train"
      ],
      "cell_type" : "markdown"
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "let data = mnistDataBunch(flat: true)"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "let (n,m) = (60000,784)\n",
        "let c = 10\n",
        "let nHid = 50"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: sgdOpt(lr: 0.1), modelInit: modelInit)\n",
        "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
        "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "learner.fit(1)"
      ],
      "outputs" : [
        
      ]
    },
    {
      "metadata" : {
        
      },
      "source" : [
        "### Mixup"
      ],
      "cell_type" : "markdown"
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "\/\/ export\n",
        "extension RandomDistribution {\n",
        "    \/\/ Returns a batch of samples.\n",
        "    func next<G: RandomNumberGenerator>(\n",
        "        _ count: Int, using generator: inout G\n",
        "    ) -> [Sample] {\n",
        "        var result: [Sample] = []\n",
        "        for _ in 0..<count {\n",
        "            result.append(next(using: &generator))\n",
        "        }\n",
        "        return result\n",
        "    }\n",
        "\n",
        "    \/\/ Returns a batch of samples, using the global Threefry RNG.\n",
        "    func next(_ count: Int) -> [Sample] {\n",
        "        return next(count, using: &ThreefryRandomNumberGenerator.global)\n",
        "    }\n",
        "}"
      ],
      "outputs" : [
        
      ]
    },
    {
      "metadata" : {
        
      },
      "cell_type" : "markdown",
      "source" : [
        "Mixup requires one-hot encoded targets since we don't have a loss function with no reduction."
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "\/\/ export\n",
        "extension Learner {\n",
        "    public class MixupDelegate: Delegate {\n",
        "        private var distribution: BetaDistribution\n",
        "        \n",
        "        public init(alpha: Float = 0.4){\n",
        "            distribution = BetaDistribution(alpha: alpha, beta: alpha)\n",
        "        }\n",
        "        \n",
        "        override public func batchWillStart(learner: Learner) {\n",
        "            if let xb = learner.currentInput {\n",
        "                if let yb = learner.currentTarget as? Tensor<Float>{\n",
        "                    var lambda = Tensor<Float>(distribution.next(Int(yb.shape[0])))\n",
        "                    lambda = max(lambda, 1-lambda)\n",
        "                    let shuffle = Raw.randomShuffle(value: Tensor<Int32>(0..<Int32(yb.shape[0])))\n",
        "                    let xba = Raw.gather(params: xb, indices: shuffle)\n",
        "                    let yba = Raw.gather(params: yb, indices: shuffle)\n",
        "                    lambda = lambda.expandingShape(at: 1)\n",
        "                    learner.currentInput = lambda * xb + (1-lambda) * xba\n",
        "                    learner.currentTarget = (lambda * yb + (1-lambda) * yba) as? Label\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    public func makeMixupDelegate(alpha: Float = 0.4) -> MixupDelegate {\n",
        "        return MixupDelegate(alpha: alpha)\n",
        "    }\n",
        "}"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "let (n,m) = (60000,784)\n",
        "let c = 10\n",
        "let nHid = 50"
      ],
      "outputs" : [
        
      ]
    },
    {
      "metadata" : {
        
      },
      "cell_type" : "markdown",
      "source" : [
        "We need to one-hot encode the targets"
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "var train1 = data.train.innerDs.map { DataBatch<TF,TF>(xb: $0.xb, \n",
        "                            yb: Raw.oneHot(indices: $0.yb, depth: TI(10), onValue: TF(1), offValue: TF(0))) }"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "var valid1 = data.valid.innerDs.map { DataBatch<TF,TF>(xb: $0.xb, \n",
        "                            yb: Raw.oneHot(indices: $0.yb, depth: TI(10), onValue: TF(1), offValue: TF(0))) }"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "let data1 = DataBunch(train: train1, valid: valid1, trainLen: data.train.dsCount, \n",
        "                  validLen: data.valid.dsCount, bs: data.train.bs)"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "func accuracyFloat(_ out: TF, _ targ: TF) -> TF {\n",
        "    return TF(out.argmax(squeezingAxis: 1) .== targ.argmax(squeezingAxis: 1)).mean()\n",
        "}"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "let learner = Learner(data: data1, lossFunc: softmaxCrossEntropy, optFunc: sgdOpt(lr: 0.1), modelInit: modelInit)\n",
        "let recorder = learner.makeRecorder()"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeShowProgress(), \n",
        "                     learner.makeAvgMetric(metrics: [accuracyFloat]), recorder,\n",
        "                     learner.makeMixupDelegate(alpha: 0.2)]"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "learner.fit(2)"
      ],
      "outputs" : [
        
      ]
    },
    {
      "metadata" : {
        
      },
      "cell_type" : "markdown",
      "source" : [
        "### Labelsmoothing"
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "\/\/ export\n",
        "@differentiable(wrt: out)\n",
        "public func labelSmoothingCrossEntropy(_ out: TF, _ targ: TI, ε: Float = 0.1) -> TF {\n",
        "    let c = out.shape[1]\n",
        "    let loss = softmaxCrossEntropy(logits: out, labels: targ)\n",
        "    let logPreds = logSoftmax(out)\n",
        "    return (1-ε) * loss - (ε \/ Float(c)) * logPreds.mean()\n",
        "}"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "@differentiable(wrt: out)\n",
        "func lossFunc(_ out: TF, _ targ: TI) -> TF { return labelSmoothingCrossEntropy(out, targ, ε: 0.1) }"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "let learner = Learner(data: data, lossFunc: lossFunc, optFunc: sgdOpt(lr: 0.1), modelInit: modelInit)\n",
        "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
        "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "learner.fit(2)"
      ],
      "outputs" : [
        
      ]
    },
    {
      "metadata" : {
        
      },
      "source" : [
        "## Export"
      ],
      "cell_type" : "markdown"
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        "import NotebookExport\n",
        "let exporter = NotebookExport(Path.cwd\/\"10_mixup_ls.ipynb\")\n",
        "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
      ],
      "outputs" : [
        
      ]
    },
    {
      "execution_count" : null,
      "metadata" : {
        
      },
      "cell_type" : "code",
      "source" : [
        
      ],
      "outputs" : [
        
      ]
    }
  ],
  "nbformat_minor" : 2,
  "nbformat" : 4,
  "metadata" : {
    "kernelspec" : {
      "language" : "swift",
      "display_name" : "Swift",
      "name" : "swift"
    }
  }
}